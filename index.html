<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Xiaomeng Hu</title>
  
  <meta name="author" content="Xiaomeng Hu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Xiaomeng Hu</name>
              </p>
              <p>I am a second-year Ph.D.student in <a href="https://www.cse.cuhk.edu.hk/">Department of Computer Science and Engineering</a>, the Chinese University of Hong Kong, supervised by <a href="https://www.cse.cuhk.edu.hk/people/faculty/tsung-yi-ho/">Prof. Tsung-Yi Ho</a>. 
                Previously I have been fortunate to work with researchers from Tsinghua University and Microsoft Research Redmond</a>.       

                <p>My research interests lie in natural language processing, deep learning, and their applications with a particular interest in building safe, trustworthy and reliable LLM systems.</p>

              <p style="text-align:center">
                <a href="mailto:hxm183083@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=u6pbsnkAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/xiaomeng-hu-greg/?originalSubdomain=hk">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/GregxmHu">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/profile.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profile.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        <!-- </tbody></table> -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="0"><tbody>
          
          <tr>
            <td style="padding-left:20px;width:20%;vertical-align:middle">   09-2023 </td>
          <td> RADAR is accepted to NeurIPS 2023. </td>
          </tr>
          <tr>
            <td style="padding-left:20px;width:20%;vertical-align:middle">   04-2022 </td>
          <td> P3 Ranker is accepted to SIGIR 2022. </td>
          </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;padding-top:40px;padding-bottom:0px;width:100%;vertical-align:middle">
            <heading>Work Experience</heading>
            <p>
            </p>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
        <td style="padding-left:20px;width:25%"> May 2023 - Nov 2023 </td>
        <td style="width:30%"> <strong><a href="https://ai.facebook.com/">FAIR, Meta AI,</a></strong></td>
      <td> Reserach Scientist Intern (Multi-Modal Fundation Models). </td>
    </tr>
    <tr>
      <td style="padding-left:20px;width:25%"> Sep 2021 - Mar 2022 </td>
      <td style="width:30%"> <strong><a href="https://research.samsung.com/aicenter_cambridge">Samsung Research AI Center</a></strong></td>
    <td> Reserach Intern (Efficient Mobile Transformers). </td>
  </tr>
  <tr>
    <td style="padding-left:20px;width:25%"> Mar 2017 - Dec 2017 </td>
    <td style="width:30%"> <strong><a href="https://www.ee.columbia.edu/ln/dvmm/">DVMM Lab, Columbia University</a></strong></td>
  <td> Reserach Assistant (Online Action Dectection). </td>
</tr>
  </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;padding-top:40px;padding-bottom:0px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;""><tbody>
          <tr onmouseout="figure_stop()" onmouseover="figure_start()">
            <td style="padding-left:20px;padding-top:50px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='figure'>
                  <img src='images/mathvision.png' width="160"></div>
                <img src='images/mathvision.png' width="160">
              </div>
              <script type="text/javascript">
                function figure_start() {
                  document.getElementById('figure').style.opacity = "1";
                }

                function figure_stop() {
                  document.getElementById('figure').style.opacity = "0";
                }
                figure_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2402.14804">
                <papertitle>Measuring Multimodal Mathematical Reasoning with the MATH-Vision Dataset
                </papertitle>
              </a>
              <br>
              K. Wang*, <b>J. Pan*</b> and W. Shi* and Z. Lu and M. Zhan and H. Li
              <br>
              <em>Arxiv</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2307.00716">arXiv</a>,
              <!-- <a href="data/2023_zero_shot_qa.bib">bibtex</a> -->
              <a href="https://mathvision-cuhk.github.io/">github</a>
              <p></p>
              <p>
                <font color=3D"crimson"> Math-Vision (Math-V), a curated collection of 3,040 high-quality mathematical problems with visual contexts sourced from real math competitions that provides a comprehensive and diverse set of challenges for evaluating the mathematical reasoning abilities of LMMs.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;""><tbody>
          <tr onmouseout="figure_stop()" onmouseover="figure_start()">
            <td style="padding-left:20px;padding-top:50px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='figure'>
                  <img src='images/journeydb.png' width="160"></div>
                <img src='images/journeydb.png' width="160">
              </div>
              <script type="text/javascript">
                function figure_start() {
                  document.getElementById('figure').style.opacity = "1";
                }

                function figure_stop() {
                  document.getElementById('figure').style.opacity = "0";
                }
                figure_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2307.00716">
                <papertitle>JourneyDB: A Benchmark for Generative Image Understanding
                </papertitle>
              </a>
              <br>
              <b>J. Pan*</b>, K. Sun*, Y. Ge, H. Li, H. Duan, X. Wu, R. Zhang, A. Zhou, Z. Qin, Y. Wang, J. Dai, Y. Qiao, H. Li
              <br>
              <em>NeurIPS</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2307.00716">arXiv</a>,
              <!-- <a href="data/2023_zero_shot_qa.bib">bibtex</a> -->
              <a href="https://github.com/JourneyDB/JourneyDB">github</a>
              <p></p>
              <p>
                <font color=3D"crimson"> JourneyDB is a large-scale generated image understanding dataset that contains 4,4M high-resolution generated images, annotated with corresponding text prompt, image caption, and visual question answering.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;""><tbody>
          <tr onmouseout="figure_stop()" onmouseover="figure_start()">
            <td style="padding-left:20px;padding-top:50px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='figure'>
                  <img src='images/zero_shot_qa.png' width="160"></div>
                <img src='images/zero_shot_qa.png' width="160">
              </div>
              <script type="text/javascript">
                function figure_start() {
                  document.getElementById('figure').style.opacity = "1";
                }

                function figure_stop() {
                  document.getElementById('figure').style.opacity = "0";
                }
                figure_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2306.11732">
                <papertitle>Retrieving-to-Answer: Zero-Shot Video Question Answering with Frozen Large Language Models
                </papertitle>
              </a>
              <br>
              <b>J. Pan</b>, Z. Lin, Y. Ge, X. Zhu, R. Zhang, Y. Wang, Y. Qiao, H. Li
              <br>
              <em>ICCVW</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2306.11732">arXiv</a>,
              <a href="data/2023_zero_shot_qa.bib">bibtex</a>
              <p></p>
              <p>
                <font color=3D"crimson"> We propose a simple yet effective Retrieving-to-Answer (R2A) framework and achieves SOTA videoQA with zero training.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;""><tbody>
          <tr onmouseout="figure_stop()" onmouseover="figure_start()">
            <td style="padding-left:20px;padding-top:50px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='figure'>
                  <img src='images/st_adapter.png' width="160"></div>
                <img src='images/st_adapter.png' width="160">
              </div>
              <script type="text/javascript">
                function figure_start() {
                  document.getElementById('figure').style.opacity = "1";
                }

                function figure_stop() {
                  document.getElementById('figure').style.opacity = "0";
                }
                figure_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2206.13559.pdf">
                <papertitle>ST-Adapter: Parameter-Efficient Image-to-Video Transfer Learning
                </papertitle>
              </a>
              <br>
              <b>J. Pan</b>, Z. Lin, X. Zhu, J. Shao, H. Li
              <br>
              <em>NeurIPS</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2206.13559">arXiv</a>, 
              <a href="data/2022_neurips.bib">bibtex</a>, 
              <a href="https://github.com/linziyi96/st-adapter">github</a>
              <p></p>
              <p>
                <font color=3D"crimson"> We propose a SpatioTemporal Adapter (ST-Adapter) for image-to-video transfer learning.
                  With ~20 times fewer parameters, we achieve on-par or better results compared to full fine-tuning strategy and state-of-theart video models.
            </p>
            </td>
          </tr>
        </tbody></table>
        <!-- <table style="width:10%;margin-left:20px"left"><tbody>
          <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=P8vcCvj_7_Z3QyyGD__x-J1xcoFUrmiib9cPIwnHKZ4"></script>        </tbody></table> -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;""><tbody>
          <tr onmouseout="figure_stop()" onmouseover="figure_start()">
            <td style="padding-left:20px;padding-top:50px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='figure'>
                  <img src='images/edgevit.png' width="160"></div>
                <img src='images/edgevit.png' width="160">
              </div>
              <script type="text/javascript">
                function figure_start() {
                  document.getElementById('figure').style.opacity = "1";
                }

                function figure_stop() {
                  document.getElementById('figure').style.opacity = "0";
                }
                figure_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2205.03436.pdf">
                <papertitle>EdgeViTs: Competing Light-weight CNNs on Mobile Devices with Vision Transformers
                </papertitle>
              </a>
              <br>
              <b>J. Pan</b>, A. Bulat, F. Tan, X. Zhu, L. Dudziak, H. Li, G. Tzimiropoulos, B. Martinez
              <br>
              <em>ECCV</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2205.03436">arXiv</a>, 
              <a href="data/2022_edgevit.bib">bibtex</a>, 
              <a href="https://github.com/saic-fi/edgevit">github</a>
              <p></p>
              <p>
                We introduce EdgeViTs, a new family of light-weight ViTs that for the first time, enable attention based vision models to compete with
                the best light-weight CNNs in the tradeoff between accuracy and on device efficiency. </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;""><tbody>
          <tr onmouseout="figure_stop()" onmouseover="figure_start()">
            <td style="padding-left:20px;padding-top:50px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='figure'>
                  <img src='images/cvpr21.png' width="160"></div>
                <img src='images/cvpr21.png' width="160">
              </div>
              <script type="text/javascript">
                function figure_start() {
                  document.getElementById('figure').style.opacity = "1";
                }

                function figure_stop() {
                  document.getElementById('figure').style.opacity = "0";
                }
                figure_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2006.07976.pdf">
                <papertitle>Actor-Context-Actor Relation Network forSpatio-Temporal Action Localization
                </papertitle>
              </a>
              <br>
              <b>J. Pan</b>*, S. Chen*, J. Shao, Z. Shou, H. Li
              <br>
              <em>CVPR</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2006.07976">arXiv</a>, 
              <a href="data/2021_cvpr.bib">bibtex</a>, 
              <a href="https://github.com/Siyu-C/ACAR-Net">github</a>
              <p></p>
              <p>
                We propose to explicitly model the Actor-Context-Actor Relation, 
                which is the relation between two actors based on their interactions with the context. Notably, our method ranks first in the AVA-Kinetics action localization task of ActivityNet Challenge 2020, outperforming other entries by a significant margin (+6.71mAP).
               </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;""><tbody>
          <tr onmouseout="figure_stop()" onmouseover="figure_start()">
            <td style="padding-left:20px;padding-top:50px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='figure'>
                  <img src='images/cvpr19.png' width="160"></div>
                <img src='images/cvpr19.png' width="160">
              </div>
              <script type="text/javascript">
                function figure_start() {
                  document.getElementById('figure').style.opacity = "1";
                }

                function figure_stop() {
                  document.getElementById('figure').style.opacity = "0";
                }
                figure_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1903.04480.pdf">
                <papertitle>Video Generation from Single Semantic Label Map
                </papertitle>
              </a>
              <br>
              <b>J. Pan</b>, C. Wang, X. Jia, J. Shao, L. Sheng, J. Yan, X. Wang
              <br>
              <em>CVPR</em>, 2019
              <br>
              <a href="https://arxiv.org/pdf/1903.04480">arXiv</a>, 
              <a href="data/2019_cvpr.bib">bibtex</a>
              <a href="https://github.com/junting/seg2vid">github</a>
              <p></p>
              <p>
                We present a two-stage framework for video synthesis conditioned on a single semantic label map.
                At the first stage, we generate the starting frame from a semantic label map. Then, we propose a
                flow prediction network to transform the initial frame to a video sequence.</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;""><tbody>
          <tr onmouseout="figure_stop()" onmouseover="figure_start()">
            <td style="padding-left:20px;padding-top:50px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='figure'>
                  <img src='images/eccv2018.png' width="160"></div>
                <img src='images/eccv2018.png' width="160">
              </div>
              <script type="text/javascript">
                function figure_start() {
                  document.getElementById('figure').style.opacity = "1";
                }

                function figure_stop() {
                  document.getElementById('figure').style.opacity = "0";
                }
                figure_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content_ECCV_2018/html/Zheng_Shou_Online_Detection_of_ECCV_2018_paper.html">
                <papertitle>Online detection of action start in untrimmed, streaming videos
                </papertitle>
              </a>
              <br>
              <b>J. Pan</b>*, Z. Shou*, J. Chan, K. Miyazawa, H. Mansour, A. Vetro, X. Giro-i-Nieto, SF. Chang
              <br>
              <em>ECCV</em>, 2018
              <br>
              <a href="https://openaccess.thecvf.com/content_ECCV_2018/html/Zheng_Shou_Online_Detection_of_ECCV_2018_paper.html">arXiv</a>, 
              <a href="data/2018_eccv.bib">bibtex</a>
              <!-- <a href="https://github.com/junting/seg2vid">github</a> -->
              <p></p>
              <p>
                We present a novel Online Detection of Action Start task in a practical setting involving untrimmed, unconstrained videos. Three training
                methods have been proposed to specifically improve the capability of ODAS models in detecting action timely and accurately.</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;""><tbody>
          <tr onmouseout="figure_stop()" onmouseover="figure_start()">
            <td style="padding-left:20px;padding-top:50px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='figure'>
                  <img src='images/cvpr2016.png' width="160"></div>
                <img src='images/cvpr2016.png' width="160">
              </div>
              <script type="text/javascript">
                function figure_start() {
                  document.getElementById('figure').style.opacity = "1";
                }

                function figure_stop() {
                  document.getElementById('figure').style.opacity = "0";
                }
                figure_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/Pan_Shallow_and_Deep_CVPR_2016_paper.pdf">
                <papertitle>Shallow and Deep Convolutional Networks for Saliency Prediction
                </papertitle>
              </a>
              <br>
              <b>J. Pan</b>*, K. McGuinness*, NE. O'Connor, E. Sayrol, X. Giro-i-Nieto          <br>
              <em>CVPR</em>, 2016
              <br>
              <a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/Pan_Shallow_and_Deep_CVPR_2016_paper.pdf">arXiv</a>, 
              <a href="data/2016_cvpr.bib">bibtex</a>
              <!-- <a href="https://github.com/junting/seg2vid">github</a> -->
              <p></p>
              <p>
               We present the first end-to-end CNN based saliency prediction network.</p>
            </td>
          </tr>
        </tbody></table>


      <table style="width:10%;margin-left:20px"left"><tbody>
        <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=fHKOjyAQM5OZpf7_YENiKNVtE2RzjQAC3yRTLPmVxuY"></script>
      </tbody></table>



  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                cr: <a href="https://github.com/jonbarron/jonbarron_website">J.Baron</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
  
</body>
</html>
